{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959e6ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#codigo para cargar los datos en 2 df, uno para el game_info y otro para las reviews\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "#esta en chunks porque mi ram no lo soportaba\n",
    "\n",
    "CHUNK_SIZE = 100000 \n",
    "def json_line_generator(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                data_object = ast.literal_eval(line)\n",
    "                yield data_object\n",
    "\n",
    "def load_data_in_chunks(file_path, chunk_size):\n",
    "    all_chunks = []\n",
    "    current_chunk = []\n",
    "    record_generator = json_line_generator(file_path)\n",
    "    for record in record_generator:\n",
    "        current_chunk.append(record)\n",
    "        if len(current_chunk) >= chunk_size:\n",
    "            all_chunks.append(pd.DataFrame(current_chunk))\n",
    "            current_chunk = [] \n",
    "    if current_chunk:\n",
    "        all_chunks.append(pd.DataFrame(current_chunk))\n",
    "\n",
    "    if all_chunks:\n",
    "        return pd.concat(all_chunks, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "file_name_games = '/content/drive/MyDrive/steam_games.json'\n",
    "df_games = load_data_in_chunks(file_name_games, CHUNK_SIZE)\n",
    "print(\"DataFrame de steam_games creado con éxito.\")\n",
    "print(\"\\nNúmero total de registros cargados:\", len(df_games))\n",
    "\n",
    "file_name_reviews = '/content/drive/MyDrive/steam_new.json'\n",
    "df_reviews = load_data_in_chunks(file_name_reviews, CHUNK_SIZE)\n",
    "print(\"DataFrame de steam_new creado con éxito.\")\n",
    "print(\"\\nNúmero total de registros cargados:\", len(df_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804038d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#poner una columna de el nombre del juego en el df de reviews\n",
    "df_games_info = df_games[['id', 'title']].rename(columns={'id': 'product_id', 'title': 'game_title'})\n",
    "df_reviews = pd.merge(\n",
    "    df_reviews,                       \n",
    "    df_games_info,                    \n",
    "    on='product_id',                  \n",
    "    how='left'                        \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cafc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8850a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#codigo para poner el texto de las reviews en minuscula, tokenizarlo y quitar stopwords\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if text is None or text == \"\":\n",
    "        return []\n",
    "    \n",
    "    #minuscula + ruido\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) #URLs\n",
    "    text = re.sub(r'<.*?>', '', text)       #etiquetas HTML\n",
    "    text = re.sub(r'[^a-záéíóúüñ\\s]', '', text) #caracteres no alfabéticos \n",
    "\n",
    "    #tokenizacion\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    #stopwords \n",
    "    spanish_stopwords = set(stopwords.words('spanish'))\n",
    "    english_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords = spanish_stopwords.union(english_stopwords)\n",
    "\n",
    "    filtered_tokens = [word for word in tokens if word not in all_stopwords and len(word) > 1]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "df_reviews['processed_tokens'] = df_reviews['text'].apply(preprocess_text)\n",
    "df_reviews['processed_text'] = df_reviews['processed_tokens'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7bf115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = df_reviews['processed_tokens'].tolist()\n",
    "\n",
    "VECTOR_SIZE = 100     \n",
    "WINDOW_SIZE = 5       \n",
    "MIN_COUNT = 5         \n",
    "WORKERS = 4           \n",
    "ITERATIONS = 10       \n",
    "\n",
    "print(\"Iniciando entrenamiento de Word2Vec...\")\n",
    "\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=sentences, \n",
    "    vector_size=VECTOR_SIZE, \n",
    "    window=WINDOW_SIZE, \n",
    "    min_count=MIN_COUNT, \n",
    "    workers=WORKERS,\n",
    "    sg=0,\n",
    "    epochs=ITERATIONS\n",
    ")\n",
    "print(f\"Vocabulario total aprendido: {len(word2vec_model.wv.index_to_key)} palabras.\")\n",
    "\n",
    "#prueba\n",
    "word = 'game'\n",
    "if word in word2vec_model.wv:\n",
    "    similares = word2vec_model.wv.most_similar(word, topn=5)\n",
    "    print(f\"\\nPalabras similares a '{word}':\")\n",
    "    for similar_word, score in similares:\n",
    "        print(f\"  - {similar_word} (Similitud: {score:.4f})\")\n",
    "else:\n",
    "    print(f\"La palabra '{word}' no se encuentra en el vocabulario (min_count={MIN_COUNT}).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
